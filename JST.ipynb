{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d172095-15b6-4c8d-ba0a-df31b9bd7355",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install jointtsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d30098-3609-4673-9140-de6f3524dd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\jh2\\\\JST'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62553c1-00c6-452f-afdd-60b4791df56b",
   "metadata": {},
   "source": [
    "### 1. 감성사전 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "255a6bd4-fbc9-414b-8f40-64720b6764e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\jh2\\lib\\site-packages\\pandas\\util\\_decorators.py:211: FutureWarning: the 'encoding' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'encoding'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# KNU 감성사전 기반 lexicon 생성 (긍정단어 - 1, 2점 -> 1점 // 부정단어 - -1, -2점 -> -1점 // 중립단어 제외)\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "sentiment_df = pd.DataFrame(index=['Word','Sentiment'])\n",
    "word_list = []\n",
    "sentiment_list = []\n",
    "\n",
    "with open('SentiWord_info.json', encoding='utf-8') as f:\n",
    "    \n",
    "    data = json.load(f)\n",
    "    \n",
    "    for i in data:\n",
    "        root = i['word_root']\n",
    "        score = int(i['polarity'])\n",
    "        \n",
    "        if score >= 1:\n",
    "            score = 1\n",
    "        \n",
    "        elif score <= -1:\n",
    "            score = -1\n",
    "        \n",
    "        elif score == 0:\n",
    "            continue\n",
    "        \n",
    "        out = pd.DataFrame({'Word':[root], 'Sentiment':[score]})\n",
    "        sentiment_df = pd.concat([sentiment_df, out])\n",
    "    \n",
    "    \n",
    "sentiment_df = sentiment_df.reset_index(drop=True).dropna()\n",
    "\n",
    "\n",
    "# 부정단어 추가\n",
    "neg_word = [\"모순\", \"고배\", \"대가리\", \"부패\", \"퇴출\", \"비판\", \"우려\", \"논란\", \"죽음\", \"비난\", \"손해\", \"지적\", \"어쩌고\", \"경고\", \"당장\", \"횡령\",\n",
    "            \"경고\", \"외면\", \"저조\", \"사기\", \"힘들다\", \"침체\", \"위협\", \"미흡\", \"정체\", \"피해자\", \"재난\", \"미안\", \"차별\", \"불매\", \"재해\", \"우려\",\n",
    "            \"참사\", \"워싱\",\"그린워싱\", \"부채\", \"낭비\", \"전쟁\", \"무시\", \"취약\", \"붕괴\", \"사태\", \"위기\", \"약화\", \"걱정\", \"미흡\", \"현실\", \"공격\"]\n",
    "neg_score = [-1]*len(neg_word)\n",
    "\n",
    "\n",
    "neg_df = pd.DataFrame(zip(neg_word, neg_score), columns=['Word', 'Sentiment'])\n",
    "\n",
    "\n",
    "\n",
    "# 긍정단어 추가\n",
    "pos_word = ['응원', \"진심\", '신뢰', '힘내', '상생', '인기', '공감', '호평', '기대'] \n",
    "pos_score = [1]*len(pos_word)\n",
    "pos_df = pd.DataFrame(zip(pos_word, pos_score), columns=['Word', 'Sentiment'])\n",
    "\n",
    "\n",
    "sentiment_df = pd.concat([sentiment_df, neg_df, pos_df], ignore_index=True)\n",
    "sentiment_df.to_excel('prior_sentiment_kor.xlsx', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb935d-dbb1-4721-affa-cb6d640b5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69481d52-e52a-4763-b4d8-419e8bc4adb3",
   "metadata": {},
   "source": [
    "### 2.전처리함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fcbebbab-c3ca-42c8-9800-9e73017f81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "#visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pylab as plb\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "\n",
    "from konlpy.tag import Mecab, Kkma, Okt\n",
    "from iteration_utilities import unique_everseen\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import font_manager, rc\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "path_list = ['C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/블로그/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/유튜브/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/인스타/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/트위터/2020_2021/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/블로그/2022',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/유튜브/2022/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/인스타/2022',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/전처리 이후/트위터/2022/',\n",
    "             'C:/Users/User/Desktop/ESG_자료/esg_텍스트/분석용/보고서/']\n",
    "\n",
    "\n",
    "def file_process(n, dt='xlsx'):\n",
    "    # 경로설정\n",
    "    path = path_list[n]\n",
    "\n",
    "    # 결과 추출을 위한 이름 설정\n",
    "    name = set_name(path)\n",
    "\n",
    "    df = open_file(path, datatype=dt)\n",
    "\n",
    "    tqdm.pandas(desc=\"Tokenization Progress\")\n",
    "    df['tokens'] = df['text'].progress_apply(preprocess, custom_dict=custom_dict)\n",
    "    df['text'] = df['text'].progress_apply(pre_process)\n",
    "\n",
    "\n",
    "    # 중복되는 df 제거\n",
    "    df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "    df['num_tokens'] = df['tokens'].map(len)\n",
    "\n",
    "    # 토큰값이 0인 행 제거\n",
    "    df = df.loc[df['num_tokens'] != 0]\n",
    "\n",
    "    # index 초기화\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "파일 불러오기를 위한 함수 설정 \n",
    "\n",
    "- path: 경로\n",
    "- datatype= 'docx' or 'xlsx'\n",
    "- min_text_docx = n 문단최소글자수 (default == 5)\n",
    "- min_text_xlsx = n 엑셀 최소 글자수 (default == 5)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def open_file(path, datatype=None, min_text_docx = 5, min_text_xlsx = 5):\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    files_xlsx = []\n",
    "    files_docx = []\n",
    "\n",
    "    if datatype == 'docx':\n",
    "        files_docx = [file for file in os.listdir(path) if file.endswith('.docx')]\n",
    "        paragraphs = []\n",
    "        titles = []\n",
    "        \n",
    "        for i in range(len(files_docx)):\n",
    "            file_name = path+files_docx[i]\n",
    "            text = docx2txt.process(file_name)\n",
    "            doc_paragraphs = [p.strip() for p in text.split('\\n'*3) if len(p.strip()) > min_text_docx]\n",
    "            # doc_paragraphs = [p.strip() for p in re.split('\\n+', text) if len(p.strip()) > min_text_docx]            \n",
    "            \n",
    "            titles += [files_docx[i]] * len(doc_paragraphs)\n",
    "            paragraphs += doc_paragraphs\n",
    "            \n",
    "        df = pd.DataFrame(paragraphs, columns=['text'])\n",
    "        # df = pd.DataFrame(list(zip(titles, paragraphs)), columns=['title', 'text'])\n",
    "\n",
    "    elif datatype == 'xlsx':\n",
    "        files_xlsx = [file for file in os.listdir(path) if file.endswith('.xlsx')]\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        if '유튜브' in path:\n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                \n",
    "                df_title = pd.read_excel(file_path, usecols=['제목'])\n",
    "                df_title = df_title.rename(columns={'제목': 'text'})   \n",
    "                df_title = df_title.drop_duplicates(keep='first')\n",
    "\n",
    "                df_text = pd.read_excel(file_path, usecols=['댓글'])\n",
    "                df_text = df_text.rename(columns={'댓글': 'text'})   \n",
    "                \n",
    "                df = pd.concat([df, df_title, df_text], ignore_index=True)\n",
    "                \n",
    "#         elif '인스타' in path:\n",
    "#             for f in files_xlsx:\n",
    "#                 file_path = os.path.join(path, f)\n",
    "                \n",
    "#                 df_text = pd.read_excel(file_path, usecols=['본문'])\n",
    "#                 df_text = df_text.rename(columns={'본문': 'text'})   \n",
    "#                 df_text = df_text.drop_duplicates(keep='first')\n",
    "\n",
    "#                 df_com = pd.read_excel(file_path, usecols=['댓글'])\n",
    "#                 df_com = df_com.rename(columns={'댓글': 'text'})  \n",
    "                \n",
    "                \n",
    "#                 df = pd.concat([df, df_text, df_com], ignore_index=True)\n",
    "\n",
    "        # 댓글 각자 문서로 처리\n",
    "    \n",
    "        elif '인스타' in path:\n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                df_text = pd.read_excel(file_path, usecols=['본문'])\n",
    "                df_text = df_text.rename(columns={'본문': 'text'})\n",
    "                df_text = df_text.drop_duplicates(keep='first')\n",
    "                df_com = pd.read_excel(file_path, usecols=['댓글'])\n",
    "                df_com = df_com.rename(columns={'댓글': 'text'})\n",
    "                \n",
    "                # 각 댓글을 하나의 문서로 분리\n",
    "                df_com['text'] = df_com['text'].str.split('/')\n",
    "                df_com = df_com.explode('text')\n",
    "                \n",
    "                df = pd.concat([df, df_text, df_com], ignore_index=True)\n",
    "\n",
    "        elif '트위터' in path:\n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                df_read = pd.read_excel(file_path, usecols=['text'])\n",
    "                df = pd.concat([df, df_read])\n",
    "\n",
    "        elif '블로그' in path:\n",
    "            \n",
    "            for f in files_xlsx:\n",
    "                file_path = os.path.join(path, f)\n",
    "                \n",
    "                # df_title = pd.read_excel(file_path, usecols=['title'])\n",
    "                # df_title = df_title.rename(columns={'title': 'text'})   \n",
    "                # df_title = df_title.drop_duplicates(keep='first')\n",
    "\n",
    "                df_text = pd.read_excel(file_path, usecols=['text'])\n",
    "                df = pd.concat([df, df_text], ignore_index=True)\n",
    "\n",
    "                # df = pd.concat([df, df_title, df_text], ignore_index=True)\n",
    "                \n",
    "\n",
    "    return df.dropna()\n",
    "\n",
    "# 형태소 분석을 위한 객체 생성\n",
    "mecab = Mecab(dicpath='c:/mecab/mecab-ko-dic')\n",
    "\n",
    "# 1차 전처리 (한글 이외에 모든 텍스트 제거)\n",
    "def pre_process(text):\n",
    "    \n",
    "    pr_text = re.sub(r\"[^ㄱ-ㅣ가-힣\\s]+|[ㄱ-ㅎㅏ-ㅣ]+\", \"\", text)\n",
    "    pr_text = re.sub('\\\\<br\\\\>',' ',pr_text)\n",
    "    pr_text = re.sub(r'\\s+', ' ', pr_text, flags=re.I)\n",
    "    pr_text = pr_text.replace(\"\\n\", \"\")\n",
    "    pr_text = pr_text.strip()\n",
    "    return pr_text\n",
    "\n",
    "# 2차 전처리 mecab을 통한 형태소 분석\n",
    "def analyzer_pos(text, custom_dict):\n",
    "    tokens = mecab.pos(text)\n",
    "    # tokens = [ s for s, t in tokens if t in ['XR', 'VV', 'VA', 'NNG', 'NNP'] and len(s) > 1 and s not in stop_word] \n",
    "    # tokens = [ s for s, t in tokens if len(s) > 1 and s not in stop_word] # 전체 품사 중 3글자 이상 + 불용어를 제외함\n",
    "    tokens = [ s for s, t in tokens if t in ['VV', 'VA', 'NNG', 'NNP'] and len(s) > 1 and s not in stop_word] # 동사, 형용사, 명사 중 2글자 이상 + 불용어를 제외함\n",
    "\n",
    "    # 원하는 단어가 끊여져서 나올경우(우선순위 설정 이후) 합치기\n",
    "    i = 0\n",
    "    new_tokens = []\n",
    "\n",
    "    while i < len(tokens):\n",
    "        if i == len(tokens) - 1:\n",
    "            new_tokens.append(tokens[i])\n",
    "            break\n",
    "        current_token = tokens[i]\n",
    "        next_token = tokens[i+1]\n",
    "        if current_token+next_token in custom_dict:\n",
    "            new_tokens.append(current_token+next_token)         \n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(current_token)\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "\n",
    "def set_name(path):\n",
    "    name = ''\n",
    "    \n",
    "    if '보고서' in path:\n",
    "        name = '보고서'\n",
    "\n",
    "    elif '인스타' in path:\n",
    "        name = '인스타'\n",
    "\n",
    "    elif '블로그' in path:\n",
    "        name = '블로그'\n",
    "\n",
    "    elif '트위터' in path:\n",
    "        name = '트위터'\n",
    "\n",
    "    elif '유튜브' in path:\n",
    "        name = '유튜브'\n",
    "    \n",
    "    return name\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "텍스트를 입력받아 사용자 정의 사전을 기반으로 정규표현식 기반 전처리, mecab 형태소 분석을 진행\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def preprocess(text, custom_dict=None):\n",
    "    \n",
    "    # 정규표현식 처리\n",
    "    text = pre_process(text)\n",
    "    \n",
    "    # mecab 전처리\n",
    "    tokens = analyzer_pos(text, custom_dict)\n",
    "    # tokens = \", \".join(tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027ed25-4c69-41d2-a678-359afa05a9a3",
   "metadata": {},
   "source": [
    "### 3. 불용어사전 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708fe24-0ef2-4f27-982f-afd4d5c98d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어\n",
    "# 길호현. (2018). 텍스트마이닝을 위한 한국어 불용어 목록 연구. 우리말글, 78, 1-25.\n",
    "\n",
    "with open(\"stop.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    stop_word = []\n",
    "    stop_text = [stop_word.extend(line.strip().split(\", \")) for line in f]\n",
    "\n",
    "    \n",
    "# with open(\"stop2.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "#     stop_word2 = []\n",
    "#     stop_text2 = [stop_word2.extend(line.strip().split(\", \")) for line in f]\n",
    "# print(stop_word2)\n",
    "\n",
    "# 개인 전처리를 위한 stop_word 설정\n",
    "stop_word_jh = ['최태원', '디스클로저', '외신', '출처', '한국', '뉴스', '최초', '공개', '단독', '회사', '상장', '보도', '글로벌', '발행', '래퍼', '출연', '리뷰', '중앙일보', '기자', '이낙연',\n",
    "                '디렉', '명상', '회장', '신문', '나라', '블로그', '코리아', '미국', '중국', '관련', '대한민국', '대리석', '타임스', '무디스', '법무법인', '변호사', '교수', '기업',\n",
    "                '타임즈', '인더스', '트리', '습니다', '에서', '속도', '한다', '이승기', '방위대', '일본', '는데', '으로', '처럼', '호남', '야구', '위한', '면서', '지만', '에게', '천억', '투데이', '해요', '펭귄', '팝콘',\n",
    "                '북마크', '네이버', '네슬레', '자칭', '네요', '코엑스', '노스페이스', '홈페이지', '어야', '전라', '전북', '매일신문', '감사', '영상', '만들', '방송', '슈카', '오늘', '배우', '세상', '사람',\n",
    "                '다니', '애플', '나오', '삼전', '민주당', '선생', '선수', '이번', '마찬가지', '그동안', '동원', '참치', '맛있', '한화', '그룹', '문재인', '래원', '노래', '좌파', '휠라', '기판',\n",
    "                '하이닉스', '한미', '입니다', '게임', '넷마블', '삼성', '시멘트', '조선', '백화점', '약품', '의약품', '해양', '효성', '렌탈', '울산', '자동차', '그램', '정답',\n",
    "                '빅이슈', '삼양', '두부', '부면', '포스코', '하림', '비빔면', '상공', '도시락', '스타', '미주', '라무', '유한', '킴벌리', '농협', '모델', '프로', '아거', '연지', '김병만',\n",
    "                '떨채', '리비', '프로', '퀴즈', '경기주택공사', '경기', '주택', '공사', '진분', '테슬라', '교보생명', '푸르', '서울', '공단', '삼성전자', '부문', '카카오', '데일리', '임팩트',\n",
    "                '국내', '시사', '센터', '앨범', '지디', '엔터', '부산', '연구원', '이벤트', '출장', '박스', '안치용', '슈가버블', '배민', '산불', '피해', '클릭', '지수', '하이브', '호비', '경기도',\n",
    "                '래리', '핑크', '기사', '링크', '윤석열', '생각', '케어', '오전', '오후', '제주', '가능', '내용', '경우', '정도', '소개', '대표', '언서', '학교', '대학교', '학생', '사진', '사용', '현대',\n",
    "                '원문']\n",
    "\n",
    "stop_word_jh2 = ['토끼', '트레저', '센트', '파리바', '마을금고', '하이트진로', '꿀벌', '부영', '전남', '아이돌', '주니어', '로봇', '요약', '사단법인', '돌아가', '멜론', '한눈', '버핏', '셋톱박스',\n",
    "                  '우편물', '현태캐피탈,', '목소리', '인사이드', '포유', '여대', '롯데제과', '김치', '대하', '홍은택', '동네', '소프트', '윤종규', '산업은행', '윤종규', '동부', '한화투자증권', '머리',\n",
    "                  '이러', '조현준', '썸머', '구내식당', '포항,', '유한양행', '제클린', '국민카드', '대우건설', '구미', '스타일', '박옥', '에버', '부천', '랜드', '프리', '선대', '아산', '한국지역난방',\n",
    "                  '테일', '이미주', '디자인', '풀무원', '매경', '롯데케미칼', '컬러,', '현대로템', '포항시', '남형', '현산', '아뉴스', '중기', '뉴스룸', '인더', '전시', '부산시', '일보', '국민은행',\n",
    "                  '성동구', '에듀', '동안', '민회', '대선', '적십자사', '수협', '진원', '파크', '한화건설', '텔레콤,', '지사', '데이', '크리에이터', '해남', '공원', '유머', '여수', '롯데카드', '케이',\n",
    "                  '특집', '현대제철', '모건', '글라스', '마사회', '인덱스', '아시아나항공', '텍스', '리더스', '파이낸셜', '스테이', '두산,', '하우', '전주시', '한국거래소', '국마', '계룡', '대한상의',\n",
    "                  '골드', '신보', '항만', '미스트', '인천공항공사', '포카', '한경', '우리금융그룹', '단백질', '축구', '제일제당', '트위터', '주세요', '파우,', '건국대', '코카', '이주현', '최중현', '중부',\n",
    "                  '퍼시픽', '연합회', '슈나이더', '드리', '코노', '아모레', '청인', '손잡', '해남군', '원장', '안랩', '로직스', '알리', '산학', '버튼', '노컷뉴스', '워크', '수력', '이제', '포스코건설,',\n",
    "                  '다회', '시립', '월드', '홍보', '박사', '강동완', '태승', '이타임즈', '강원도', '코람코', '석화', '신용보증기금', '남양주', '데모', '독스', '헬스', '용가리동아', '인수위', '그러', '이마트',\n",
    "                  '일렉트릭', '조원', '은행장,', '위아', '알아보', '코레', '홈플러스', '바디스', '이윤진', '닷컴', '위메이드', '전경련', '맥주', '고은정', '스리랑카', '그렇', '신지현', '이노', '끝내',\n",
    "                  '캠프', '회차', '안동', '레포트', '지오', '헤럴드', '유업', '이카', '진천', '단심', '유업', '이카','피플', '고배', '이카', '출판사', '퇴계원', '엡손', '광명시', '난지', '아줌마', '주간경향', '버티',\n",
    "                  '달수', '아빠', '언론사', '특별시', '차기', '대상', '브레이크', '제회', '플러스', '후계자', '골프', '연예', '인천', '무료', '러시아', '의원', '수원', '서부', '이종현', '준원', '케미',\n",
    "                  '지회', '강자', '남양주시', '비즈', '손해', '연합뉴스', '김주현', '피니언', '실시간', '스포츠']\n",
    "\n",
    "stop_word.extend(stop_word_jh)\n",
    "stop_word.extend(stop_word_jh2)\n",
    "\n",
    "print(stop_word)\n",
    "print(len(stop_word))\n",
    "\n",
    "custom_dict = ['지속가능', '이해관계자', '탄소중립', '신재생에너지', '재생에너지', '탄소중립', '탄소제로', '메타버스', '지배구조', '지속가능보고서', '지속가능경영보고서', '사외이사',\n",
    "               '이상기후', '기후변화', '게임체인저', '동반성장', '고부가가치', '협력사', '가치사슬', '밸류체인', '유연근무', '정보보호', '이행원칙', '노사협의회', '위드코로나', '제로웨이스트',\n",
    "               '제로웨이스트', '위드코로나', '암호화폐', '그린워싱', '배당귀족']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea84702-f9ff-41f6-8682-bf16204652ab",
   "metadata": {},
   "source": [
    "### 4. 데이터 호출 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "96b6315c-d5b3-43b4-af27-8424f542c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization Progress: 100%|███████████████████████████████████████████████████| 21001/21001 [00:02<00:00, 7681.02it/s]\n",
      "Tokenization Progress: 100%|█████████████████████████████████████████████████| 21001/21001 [00:00<00:00, 121095.10it/s]\n",
      "C:\\Users\\User\\anaconda3\\envs\\jh2\\lib\\site-packages\\pandas\\util\\_decorators.py:211: FutureWarning: the 'encoding' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'encoding'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import docx2txt\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "path_list (블로그/유튜브/인스타/트위터) + 보고서\n",
    "[0~3] - 2020/2021  0:블로그 / 1: 유튜브 / 2: 인스타 / 3: 트위터\n",
    "[4~7] - 2022       4:블로그 / 5: 유튜브 / 6: 인스타 / 7: 트위터\n",
    "[8]   - 보고서      8: 보고서\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame()\n",
    "# df = file_process(4)\n",
    "\n",
    "# df_blog = file_process(4)\n",
    "# df_yt = file_process(5)\n",
    "# df_insta = file_process(6)\n",
    "# df_twitter = file_process(7)\n",
    "\n",
    "df = file_process(7)\n",
    "\n",
    "# df = pd.concat([df, df_blog, df_yt, df_insta, df_twitter], ignore_index=True)\n",
    "df.to_excel('소셜미디어_전처리.xlsx', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c021172-1136-492e-acde-51114684e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 확인\n",
    "with open(\"C:\\\\mecab\\\\user-dic\\\\nnp.csv\", 'r', encoding='utf-8') as f: \n",
    "    file_data = f.readlines()\n",
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf91672-2e34-4d24-98a6-88469ad6d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우선순위 확인\n",
    "with open(\"c:/mecab/mecab-ko-dic/user-nnp.csv\", 'r', encoding='utf-8') as f: \n",
    "    file_data = f.readlines()\n",
    "file_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc5ff125-c381-44bb-af5a-64ca355be000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "60cf33d6-a66c-472b-8031-d33fd23356ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import ast\n",
    "\n",
    "\"\"\"\n",
    "데이터 호출 & 전처리 결과 호출\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df= pd.read_excel('소셜미디어_전처리.xlsx', dtype={'tokens': list})\n",
    "df['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820f5d8-7db9-4dd7-a08f-e6855c6d9ea3",
   "metadata": {},
   "source": [
    "### 4.1. 토큰 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e378369-f922-4896-8bef-ed2507e1a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('모르', '모르다') for i in x])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('줄이', '줄이다') for i in x])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('재밌', '재밌다') for i in x])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('밸류체인', '가치사슬') for i in x])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('어쩌고', '어쩌구') for i in x])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('어떻', '어떻게') for i in x])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('힘드', '힘들다') for i in x])\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [i.replace('만드', '만들다') for i in x])\n",
    "\n",
    "# 대상 단어들\n",
    "da_list = ['모르', '빠르', '재밌', '미치', '대하', '보내', '바라보', '떨어지', '떠오르', '지키', '힘들', '모르', '맞추', '이끌', '구하', '낮추', '없애', '따지', '애쓰', '커지', '늘리', '키우',\n",
    "           '살리', '앞두']\n",
    "\n",
    "# 데이터프레임의 'tokens' 열에 적용할 함수\n",
    "def replace_word_with_da(word):\n",
    "    if word in da_list:\n",
    "        return word + '다'\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# 데이터프레임에 함수 적용하기\n",
    "df['tokens'] = df['tokens'].apply(lambda x: [replace_word_with_da(word) for word in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997cc4f2-aa34-486f-8b4c-5a529515b47f",
   "metadata": {},
   "source": [
    "### 5.JST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb2e1f84-93cb-4658-bd17-3b7a28bf89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jointtsmodel.RJST import RJST\n",
    "from jointtsmodel.JST import JST\n",
    "from jointtsmodel.sLDA import sLDA\n",
    "from jointtsmodel.TSM import TSM\n",
    "from jointtsmodel.TSWE import TSWE\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from jointtsmodel.utils import *\n",
    "\n",
    "\n",
    "text = df['tokens'].apply(lambda x: ' '.join(x))\n",
    "vectorizer = CountVectorizer(max_df=0.5,\n",
    "                             min_df=3,\n",
    "                             # ngram_range=(1,2),          #바이그램 사용시 성능 확인\n",
    "                             \n",
    "                             max_features=3000)\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "inv_vocabulary = dict(zip(vocabulary,np.arange(len(vocabulary))))\n",
    "lexicon_data = pd.read_excel('./prior_sentiment_kor.xlsx')\n",
    "lexicon_data = lexicon_data.dropna()\n",
    "lexicon_dict = dict(zip(lexicon_data['Word'],lexicon_data['Sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3fbd5b86-5bf8-4c86-af6d-93aa941f6f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity after iteration 10 (out of 30 iterations) is 2474.03\n",
      "Perplexity after iteration 20 (out of 30 iterations) is 2033.29\n",
      "Perplexity after iteration 30 (out of 30 iterations) is 1951.69\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20357482701902432"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# for n in range(3, 16):\n",
    "\n",
    "model = RJST(n_topic_components=3, n_sentiment_components=2, random_state=123,evaluate_every=10, max_iter=30)\n",
    "model.fit(X.toarray(), lexicon_dict)\n",
    "\n",
    "# model.transform()[:2]\n",
    "\n",
    "top_words = list(model.getTopKWords(vocabulary).values())\n",
    "\n",
    "'''\n",
    "utils.py // def coherence_score_umass() 깃허브 확인 후 코드 수정 (127~129) 2023-03-19\n",
    "\n",
    "'''\n",
    "# coherence_score_umass(X.toarray(),inv_vocabulary,top_words)\n",
    "coherence_score_uci(X.toarray(),inv_vocabulary,top_words)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9ae8bfc-8112-45d2-9ea9-150a3504ff3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(1, 1): ['경영', '실천', '사회', '개최', '지원', '위원회', '사업', '활동', '수상', '지역'],\n",
       " (1, 2): ['투자', '사회', '세계', '에너지', '펀드', '공시', '시장', '시대', '중요', '주식'],\n",
       " (2, 1): ['경영', '금융', '은행', '진행', '공모전', '발표', '경제', '포럼', '모집', '전략'],\n",
       " (2, 2): ['경영', '평가', '강화', '등급', '발간', '금융', '기관', '획득', '건설', '보고서'],\n",
       " (3, 1): ['경영', '테크', '트렌드', '협약', '지원', '체결', '혁신', '중소기업', '업무', '스타트업'],\n",
       " (3, 2): ['환경', '친환경', '경영', '사회', '탄소', '캠페인', '탄소중립', '기후', '경제', '활용']}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.getTopKWords(ㅊ, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9f199-faca-43a1-af84-992060ac51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수정사항 확인용\n",
    "import inspect\n",
    "print(inspect.getsource(coherence_score_umass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b0dbc21-7b0c-4600-9772-3fe0ad6d2b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1651.2570725141406"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perplexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "25d467da-8af7-497e-a97b-74c31c0c81a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = list(model.getTopKWords(vocabulary).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b3c4d9-0157-44bc-9a12-928c2798944a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
